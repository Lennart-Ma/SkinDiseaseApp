{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b71a5195-09ea-4381-99cc-e01fa6131c3f","_uuid":"d994d38b-4fcd-4b79-9bfc-2706ef569d5c","trusted":true},"source":["# **Melanoma Detection** - Training was performed in a kaggle notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade efficientnet-pytorch\n","!pip install pretrainedmodels\n","!pip install wtfml"]},{"cell_type":"markdown","metadata":{},"source":["## Create new ground truth file for new image data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","\n","path = r\"../input/isic-2019-test-input-512/ISIC_2019_Test_Input_512\" # Path to the folder where we store all images for which we want to create the new ground truth file!\n","\n","csv_df_export_path = r\"./test_gt_empty_sorted.csv\" # Output path of the csv file\n","\n","\n","#############################################################################################################\n","\n","\n","image_names = []\n","\n","for i,file_name in enumerate(glob.glob(os.path.join(path, \"*jpg\"))):\n","    basename = os.path.basename(file_name)[:-4]\n","    image_names.append(basename)    \n","    \n","tryout_df = pd.DataFrame(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n","\n","tryout_df.insert(loc=0, column=\"image\", value=image_names)\n","\n","tryout_df = tryout_df.fillna(0)\n","\n","tryout_df_sorted = tryout_df.sort_values(\"image\")\n","\n","tryout_df_sorted.to_csv(csv_df_export_path, index=False)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"141b9eab-c27e-4673-b05b-00f7492b97d6","_uuid":"efb88c7e-b82a-4230-ad6f-3673e36a1aec","trusted":true},"source":["## Hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d476ab45-a615-4a56-8e99-537d159d7fb5","_uuid":"0cbf18db-8af3-40b4-a95f-5c06e2d583ae","trusted":true},"outputs":[],"source":["\n","\"\"\"You need to split the image data (which you downloaded from the ISIIC website) according to the gt_train and gt_val csv files into two folders\n","The paths to these folders need to be put into the variables input_path_training_img and input_path_val_img. Spliting the image data can be done \n","with val_split.py\n","\n","Additionally, there is also a resize.py file in the gitlab repository, which resizes your images to the wanted size.\n","\n","If you have question regarding this CNN script, the val_split.py file or the resize.py file, just text me.\n","\n","\"\"\" \n","\n","\n","input_path_training_img = \"../input/image512/Train_Data_2019_512/Train_Data_2019_512\"\n","input_path_val_img = \"../input/image512/Val_Data_2019_512/Val_Data_2019_512\"\n","\n","input_path_predict_img = \"../input/isic-2019-test-input-512/ISIC_2019_Test_Input_512\"\n","\n","\n","# Height and width of the input images\n","H = 512\n","W = 512\n","\n","path_train_gt = \"../input/train5gt/train_5_folds.csv\"\n","path_val_gt = \"../input/melanoma-detection-data/groundtruth_val.csv\"\n","\n","path_predict_gt = \"../input/test-gt-empty-sorted/test_gt_empty_sorted.csv\"\n","\n","\n","num_epochs = 9\n","\n","batch_size = 8\n","\n","patience_for_early_stopping = 5\n","\n","learning_rate = 0.0001"]},{"cell_type":"markdown","metadata":{"_cell_guid":"7abf7bfe-09e6-4b11-aee1-ee752505c0ab","_uuid":"f7038f39-ecea-4b4b-9b2d-452f7db55509","trusted":true},"source":["## Create a Dataset class\n","The Dataloader needs a Pytorch Dataset format as an input. This Dataset format is created here. As input you take the gt_path and image_path"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73a33aab-bdc3-48af-81b0-e01359d3e9fa","_uuid":"1031ae80-2c84-4f24-ace0-1e58023140be","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision import transforms, utils\n","import matplotlib.pyplot as plt\n","from skimage import io, transform\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import torchvision.transforms as transforms\n","from wtfml.utils import EarlyStopping\n","\n","\n","# Load Dataset\n","class Melanoma_Dataset(Dataset):\n","    def __init__(self,gt_path, image_path, transform = None, val = False):\n","        \n","        self.groundtruth = pd.read_csv(gt_path)\n","        self.image_path = image_path\n","        self.transform = transform\n","        \n","        self.composed = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","        \n","    def __len__(self):\n","        return len(self.groundtruth)\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","            \n","        img_name = os.path.join(self.image_path,\n","                               self.groundtruth.iloc[idx,0])\n","        image = io.imread(img_name+\".jpg\")\n","        # labels = self.groundtruth.iloc[idx, 1:10]\n","        labels = torch.tensor(self.groundtruth.iloc[idx, 1:10].astype(int))\n","        # labels = np.array([labels])\n","        \n","        \n","        if self.transform:\n","            image = self.transform(image)\n","            \n","        \n","        return image, labels"]},{"cell_type":"markdown","metadata":{"_cell_guid":"18796732-e340-4ed8-ab06-0f238eadc862","_uuid":"8af91841-5abc-449c-806b-a8cf3a43ad80","trusted":true},"source":["## CNN"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2db1d44e-6267-4aff-8833-546b218cd05b","_uuid":"953a0b4f-abd4-44a0-aef6-061eb0dfed99","trusted":true},"source":["### Explanation of the CNN Architecture\n","**ImageHeightxImageWidth** (Input to EfficientNet) --> **2560,10,10** (Output of EfficientNet) -(GlobalAvgPooling)-> **2560** (Input for LinearFullyConnected Layer) -> **9** (Output of the Net)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a42cfd98-1a62-41ad-9739-3c7a31d88f1f","_uuid":"afc2646f-670d-462b-b2e4-e7b0e725642e","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from efficientnet_pytorch import EfficientNet\n","from torch.nn import functional as F\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn import metrics\n","import pretrainedmodels\n","\n","class SEResnext50_32x4d(nn.Module):\n","    def __init__(self, pretrained= None):\n","        super(SEResnext50_32x4d, self).__init__()\n","        \n","        self.base_model = pretrainedmodels.__dict__[\n","            \"se_resnext50_32x4d\"\n","        ](pretrained=None)\n","        if pretrained is not None:\n","            self.base_model.load_state_dict(\n","                torch.load(\n","                    \"../input/pretrained-model-weights-pytorch/se_resnext50_32x4d-a260b3a4.pth\"\n","                )\n","            )\n","\n","        self.dense_output = nn.Linear(2048, 9)\n","    \n","    def forward(self, image):\n","        batch_size, _, _, _ = image.shape\n","        \n","        x = self.base_model.features(image)\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n","\n","        x = self.dense_output(x)\n","\n","        return x\n","\n","def GlobalAveragePooling(x):\n","    return x.mean(axis=-1).mean(axis=-1)\n","\n","class MelanomaNet(nn.Module):\n","    def __init__(self, pretrained = None):\n","        super(MelanomaNet, self).__init__()\n","        if pretrained is None:\n","            self.efn = EfficientNet.from_pretrained(\"efficientnet-b7\")\n","        else:\n","            self.efn = EfficientNet.from_name(\"efficientnet-b7\")\n","            self.efn.load_state_dict(torch.load(pretrained), strict = False)\n","        \n","        # Classifier\n","        self.avgpool = GlobalAveragePooling\n","        self.dense_output = nn.Linear(2560, 9) # length of output vector from EfficientNetb7 is 2560\n","        # self.fc2a = nn.Linear(500, 9)\n","        \n","                \n","    def forward(self, x):\n","        x = x.view(-1, 3, H , W) # has to be done since EfficientNet is taking such an input\n","        x = self.efn.extract_features(x)\n","        x = self.avgpool(x)\n","        x = self.dense_output(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"_cell_guid":"dd81c429-d0c4-4f1f-be8c-b5e5fe79e359","_uuid":"0dcaf099-7aea-492b-a52a-bc7ba1a047ff","trusted":true},"source":["## Defining the training and validation function"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b64660a1-1d1c-4787-a4c6-bae06dbd4865","_uuid":"6341255c-19d1-4d6f-9e33-2de707c3a40b","trusted":true},"outputs":[],"source":["def train(model, device, train_loader, optimizer, loss_function, epoch):\n","    \n","    # Set to training mode\n","    model.train()\n","    \n","    # Loop over all examples\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","\n","        # Push to GPU\n","        data, target = data.to(device), target.to(device)\n","\n","        #Reset gradients\n","        optimizer.zero_grad()\n","\n","        with torch.set_grad_enabled(True):\n","\n","            #Calculate outputs\n","            output = model(data)\n","\n","\n","            # print(\"output:\", output)\n","\n","\n","            # print(\"torch.max(target,1)[1]:\", torch.max(target, 1)[1])\n","\n","            # Calculate loss (is this correct??)\n","            loss = loss_function(output, torch.max(target, 1)[1])\n","\n","            # Backpropagate loss\n","            loss.backward()\n","\n","            # Apply gradients\n","            optimizer.step()\n","\n","\n","        # printout every 50 batches\n","        if batch_idx % 50 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","             \n","            \n","            \n","def val(model, device, test_loader, loss_function, prediction_return=False):\n","    # Set to evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            curr_loss = loss_function(output,torch.max(target, 1)[1])\n","            output = output.cpu().numpy()\n","            if i==0:\n","                predictions = output\n","                \n","                targets = target.data.cpu().numpy()\n","                \n","                loss = np.array([curr_loss.data.cpu().numpy()])\n","            else:\n","                predictions = np.concatenate((predictions,output))\n","                targets = np.concatenate((targets,target.data.cpu().numpy()))\n","                loss = np.concatenate((loss,np.array([curr_loss.data.cpu().numpy()])))\n","    # One-hot to index:\n","    \n","    predictions_values = predictions\n","    \n","    predictions = np.argmax(predictions, 1)\n","    \n","    targets = np.argmax(targets, 1)\n","    \n","    # Caluclate metrics\n","    accuracy = np.mean(np.equal(predictions,targets))\n","    conf_mat = confusion_matrix(targets,predictions)\n","    \n","    sensitivity = conf_mat.diagonal()/conf_mat.sum(axis=1) # taking the mean calculates the mean sensitivity over ALL classes\n","    \n","    try:\n","        sensitivity_mel = conf_mat[0][0]/conf_mat[0].sum()\n","    except:\n","        sensitivity_mel = \"No Value\"\n","    \n","    # Print metrics\n","    # print(classification_report(targets, predictions, digits=3))\n","    print(\"Test Accuracy:\",accuracy,\"Test Sensitivity (Overall):\",np.mean(sensitivity), \"MEL Sensitivity:\", sensitivity_mel, \"Test loss:\",np.mean(loss))\n","    \n","    if prediction_return == True:\n","        print(\"prediction_return = True\")\n","    \n","    return predictions, predictions_values, targets\n","\n","\n","\n","def predict(model, device, test_loader, loss_function, prediction_return=False):\n","    # Set to evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (data, _) in enumerate(test_loader):\n","            data= data.to(device)\n","            output = model(data)\n","            # curr_loss = loss_function(output,torch.max(target, 1)[1])\n","            output = output.cpu().numpy()\n","            if i==0:\n","                predictions = output\n","                \n","            else:\n","                predictions = np.concatenate((predictions,output))\n","    \n","    # predictions = np.argmax(predictions, 1)\n","    \n","    \n","    return predictions"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ad5de5cd-e852-4741-998f-5db9bde0579c","_uuid":"1eade444-f90b-4b4e-9f27-09fde123e491","trusted":true},"source":["## Creating the Actual Datasets for the Dataloader"]},{"cell_type":"markdown","metadata":{},"source":["RandomHorizontalFlip(Can be a PIL Image or torch Tensor), RandomVerticalFlip(Can be a PIL Image or torch Tensor), RandomRotation(Can be a PIL Image or torch Tensor),\n","transforms.ColorJitter(Can be a PIL Image or torch Tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d32cd5f-8f98-445d-9441-5b8272509bf1","_uuid":"8ef200af-ba8f-4fbe-8029-51fdcedfb67e","trusted":true},"outputs":[],"source":["\n","mean = [0.485, 0.456, 0.406]\n","\n","std = [0.229, 0.224, 0.255]\n","\n","\n","\n","# Here we determine what transformation/augmentations we want to have on our image (e.g. flipping, rotating etc.)\n","train_transform = transforms.Compose([transforms.ToPILImage(),\n","                                      transforms.RandomHorizontalFlip(p=0.5),\n","                                      transforms.RandomVerticalFlip(p=0.5),\n","                                      transforms.RandomApply([transforms.RandomRotation(180), transforms.ColorJitter()], p =0.4),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.255))\n","                                     ])\n","\n","\n","val_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.255))])\n","\n","\n","\n","\n","\n","# you have to take the path to \"Test_Data_2019_384\" - I misspelled it\n","train_dataset = Melanoma_Dataset(gt_path = path_train_gt,\n","                                image_path = input_path_training_img,\n","                                transform = train_transform)\n","\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size,\n","                                           shuffle = True)\n","\n","\n","# you have to take the path to \"Test_Data_2019_384\" - I misspelled it\n","val_dataset = Melanoma_Dataset(gt_path = path_val_gt,\n","                                image_path = input_path_val_img,\n","                                transform = val_transform, val = False)\n","\n","\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size,\n","                                           shuffle = False)\n","\n","\n","predict_dataset = Melanoma_Dataset(gt_path = path_predict_gt,\n","                                  image_path = input_path_predict_img,\n","                                  transform = val_transform, val = False)\n","\n","predict_loader = torch.utils.data.DataLoader(dataset = predict_dataset, batch_size = batch_size,\n","                                           shuffle = False)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"282d592a-fdc9-41be-b2b8-392daa940f1b","_uuid":"3a9f0a8a-7337-4e3c-beb3-aab24182ad51","trusted":true},"source":["## Preperation for the actual training"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa6b118f-9049-4afa-b41b-40b3ce622926","_uuid":"08931027-9873-4b56-a55a-ee1d7b27a698","trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","#GPU Info\n","print('Numbers of GPUs: ', torch.cuda.device_count())\n","print('Type of GPUs: ', torch.cuda.get_device_name(device=None))\n","\n","# Define device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Model\n","# model = MelanomaNet(pretrained = None).to(device)\n","\n","model = SEResnext50_32x4d()\n","\n","#model_path = \"../input/model-submission-8/model_8.bin\"\n","#model.load_state_dict(torch.load(model_path))\n","#model.to(device)\n","\n","\n","\n","# Loss\n","loss_function = nn.CrossEntropyLoss() #should only be used for 1 class problems??\n","# loss_function = nn.NLLLoss()\n","\n","# Optimizer / we could implement an adaptive learning rate\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","\n","# Learning Rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, mode=\"max\") #we schedule on the auc of roc curve \n","\n","# Early Stopping Library\n","early_stopping = EarlyStopping(patience_for_early_stopping, mode=\"max\")\n","\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"82d26bd5-be54-4dc4-b1db-6adef2c3be3c","_uuid":"fcdeecf8-d3a0-4e42-bf25-30d35cc11ec3","trusted":true},"source":["## Where we start training and validate"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7308e9c0-b9d2-4c30-a598-1ab2e86bbd0b","_uuid":"482ca47a-a4e9-42b6-bc96-0089c2bf7eae","trusted":true},"outputs":[],"source":["n_total_steps = len(train_loader)\n","\n","prediction_return = True\n","\n","print(n_total_steps)\n","\n","for epoch in range(num_epochs):\n","    train(model, device, train_loader, optimizer, loss_function, epoch)\n","        \n","    predictions, predictions_values, targets = val(model, device, val_loader, loss_function)\n","    \n","    ################\n","    # Normalize predictions_values for auc\n","    # Das hier ausgliedern in neue Funktion!\n","    pred_val_del = np.delete(predictions_values, -1, 1) # we have to delete the last entry of all predictions since the index 8 is never appearing in the targets file\n","    \n","    pred_val_torch = torch.from_numpy(pred_val_del)\n","    pred_val_sig = torch.sigmoid(pred_val_torch)\n","    row_sums  = torch.sum(pred_val_sig,1)\n","    row_sums_full = torch.repeat_interleave(row_sums,8)\n","    divider = torch.reshape(row_sums_full, (len(predictions_values),8))\n","    y_pred = torch.div( pred_val_sig , divider )\n","    y_pred_np = y_pred.numpy()\n","\n","    ################\n","    \n","    auc = metrics.roc_auc_score(targets, y_pred_np, multi_class = \"ovr\")\n","    \n","    print(f\"Epoch = {epoch}, AUC = {auc}\")\n","    \n","    lr_scheduler.step(auc)\n","    \n","    early_stopping(auc, model, model_path=f\"model_{epoch}.bin\")\n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Load and use a saved model for prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict_SEResnext50():\n","    model_path = \"../input/model-submission-8/model_8.bin\"\n","    \n","    model_pred = SEResnext50_32x4d(pretrained=None)\n","    model_pred.load_state_dict(torch.load(model_path))\n","    model_pred.to(device)\n","    \n","    new_predictions = predict(model_pred, device, val_loader, loss_function)\n","    # new_predictions = val(model_pred, device, val_loader, loss_function)\n","    \n","    return new_predictions\n","    \n","    \n","def predict_EfficientNet():\n","    \n","    model_path = \"../input/model-7/model_7.bin\"\n","    \n","    model_pred = MelanomaNet(pretrained = model_path)\n","    model_pred.to(device)\n","    \n","    new_predictions, _, _ = val(model_pred, device, val_loader, loss_function)\n","    \n","    return new_predictions\n","\n","\n","new_predictions = predict_SEResnext50()\n","# new_predictions = predict_EfficientNet()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(type(new_predictions))\n","print(len(new_predictions))\n","print(new_predictions)\n","prediction_values = new_predictions"]},{"cell_type":"markdown","metadata":{"_cell_guid":"07f146eb-8c6e-4eb6-8c30-4f7e335625e9","_uuid":"53883573-80b8-4625-b5e4-8efe1ff3eed8","trusted":true},"source":["# Create the submission csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# USE new_predictions variable from code block above to create the csv from the predictions made from an uploaded model!!!\n","t_p_a = new_predictions\n","\n","output_df = pd.read_csv(\"../input/test-gt-empty-sorted/test_gt_empty_sorted.csv\") # VERY IMPORTANT PUT THE GROUND TRUTH CSV FILE PATH WHICH YOU USE FOR THE DATALOADER IN HERE!!!!\n","image_names = output_df.drop(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n","\n","\n","pred_df = pd.DataFrame(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n","\n","\n","for i in range(len(t_p_a)):\n","\n","    pred_image = np.array([0,0,0,0,0,0,0,0,0])\n","    np.put(pred_image, t_p_a[i],1)\n","    pred_df = pred_df.append({\"MEL\": pred_image[0], \"NV\": pred_image[1], \"BCC\": pred_image[2],\n","                              \"AK\": pred_image[3], \"BKL\": pred_image[4], \"DF\": pred_image[5],\n","                              \"VASC\": pred_image[6], \"SCC\": pred_image[7], \"UNK\": pred_image[8]},\n","                             ignore_index=True\n","                              )    \n","\n","pred_df.insert(loc=0, column=\"image\", value=image_names)\n","\n","submission = pred_df\n","\n","submission.to_csv(\"phase2_submission_group_4.csv\", index=False) # Put the name to the output path as the first parameter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","image_names = [\"image\"]\n","\n","dataframe = pd.DataFrame(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n","\n","dataframe.insert(loc=0, column=\"image\", value=image_names)\n","\n","dataframe = dataframe.fillna(0)\n","\n","dataframe.to_csv(\"single_image_predict.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Print the ROC Curve"]},{"cell_type":"markdown","metadata":{},"source":["### how they did it in the tutorial (https://scikit-learn.org/0.15/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)"]},{"cell_type":"markdown","metadata":{},"source":["**Ablauf:**\n","\n","    1. Lösche aus predictions_values den letzten Eintrag (UNK, 9ter Eintrag, [8]) --> del_pred\n","    2. Bringe predictions_values auf die Form (0.6,0.04,0.02,0.18,0.01,0.3,0.01,0.011) alle values zusammen müssen 1 ergeben!\n","        a. Das ist unser y_score\n","    3. Nutze die gt_val data und bringe sie auf die Form: np.array (1,0,0,0,0,0,0,0)\n","        b. Das ist unser y_test\n","        \n","    4. n_classes = 8 (UNK wird rausgekickt)\n","    \n","    5. Code wie ein Block weiter oben bzw. wie auf der sklearn Seite ausführen"]},{"cell_type":"markdown","metadata":{},"source":["## argmax to one hot encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import scipy\n","pred = np.delete(prediction_values, -1, 1) \n","print(pred[0])\n","\n","pred_prob = scipy.special.softmax(pred, axis = 1)\n","print(pred_prob[0])\n","pred_prob.sum(axis= 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del_pred = np.delete(predictions_values, -1, 1)  \n","del_pred_max = np.argmax(del_pred, axis = 1)\n","\n","\n","del_pred_one_hot_enc = np.zeros((del_pred_max.size, del_pred_max.max()+1))\n","del_pred_one_hot_enc[np.arange(del_pred_max.size),del_pred_max] = 1\n","\n","y_score = del_pred_one_hot_enc"]},{"cell_type":"markdown","metadata":{},"source":["## gt_val data from csv to numpy one hot encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gt_df = pd.read_csv(\"../input/melanoma-detection-data/groundtruth_val.csv\")\n","\n","del gt_df[\"image\"]\n","\n","gt_np = gt_df.to_numpy()\n","\n","y_test = gt_np\n","\n","y_test[:, 3]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm, datasets\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","classes = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\"]\n","\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","\n","gt_df = pd.read_csv(\"../input/melanoma-detection-data/groundtruth_val.csv\")\n","del gt_df[\"image\"]\n","gt_np = gt_df.to_numpy()\n","y_test = gt_np\n","y_test[:, 3]\n","\n","y_score = pred\n","\n","\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(len(classes)):\n","    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","for i in range(len(classes)):\n","    plt.plot(fpr[i], tpr[i], label=\"ROC curve of class \" + classes[i] + \"(area = {1:0.2f})\"\n","                                   ''.format(i, roc_auc[i]))\n","    \n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
