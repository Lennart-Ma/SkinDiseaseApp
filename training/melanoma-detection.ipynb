{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Melanoma Detection**","metadata":{"_uuid":"d994d38b-4fcd-4b79-9bfc-2706ef569d5c","_cell_guid":"b71a5195-09ea-4381-99cc-e01fa6131c3f","trusted":true}},{"cell_type":"code","source":"!pip install --upgrade efficientnet-pytorch\n!pip install pretrainedmodels\n!pip install wtfml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create new ground truth file for new image data","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\n\npath = r\"../input/isic-2019-test-input-512/ISIC_2019_Test_Input_512\" # Path to the folder where we store all images for which we want to create the new ground truth file!\n\ncsv_df_export_path = r\"./test_gt_empty_sorted.csv\" # Output path of the csv file\n\n\n#############################################################################################################\n\n\nimage_names = []\n\nfor i,file_name in enumerate(glob.glob(os.path.join(path, \"*jpg\"))):\n    basename = os.path.basename(file_name)[:-4]\n    image_names.append(basename)    \n    \ntryout_df = pd.DataFrame(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n\ntryout_df.insert(loc=0, column=\"image\", value=image_names)\n\ntryout_df = tryout_df.fillna(0)\n\ntryout_df_sorted = tryout_df.sort_values(\"image\")\n\ntryout_df_sorted.to_csv(csv_df_export_path, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter","metadata":{"_uuid":"efb88c7e-b82a-4230-ad6f-3673e36a1aec","_cell_guid":"141b9eab-c27e-4673-b05b-00f7492b97d6","trusted":true}},{"cell_type":"code","source":"\n\"\"\"You need to split the image data (which you downloaded from the ISIIC website) according to the gt_train and gt_val csv files into two folders\nThe paths to these folders need to be put into the variables input_path_training_img and input_path_val_img. Spliting the image data can be done \nwith val_split.py\n\nAdditionally, there is also a resize.py file in the gitlab repository, which resizes your images to the wanted size.\n\nIf you have question regarding this CNN script, the val_split.py file or the resize.py file, just text me.\n\n\"\"\" \n\n\ninput_path_training_img = \"../input/image512/Train_Data_2019_512/Train_Data_2019_512\"\ninput_path_val_img = \"../input/image512/Val_Data_2019_512/Val_Data_2019_512\"\n\ninput_path_predict_img = \"../input/isic-2019-test-input-512/ISIC_2019_Test_Input_512\"\n\n\n# Height and width of the input images\nH = 512\nW = 512\n\npath_train_gt = \"../input/train5gt/train_5_folds.csv\"\npath_val_gt = \"../input/melanoma-detection-data/groundtruth_val.csv\"\n\npath_predict_gt = \"../input/test-gt-empty-sorted/test_gt_empty_sorted.csv\"\n\n\nnum_epochs = 9\n\nbatch_size = 8\n\npatience_for_early_stopping = 5\n\nlearning_rate = 0.0001","metadata":{"_uuid":"0cbf18db-8af3-40b4-a95f-5c06e2d583ae","_cell_guid":"d476ab45-a615-4a56-8e99-537d159d7fb5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a Dataset class\nThe Dataloader needs a Pytorch Dataset format as an input. This Dataset format is created here. As input you take the gt_path and image_path","metadata":{"_uuid":"f7038f39-ecea-4b4b-9b2d-452f7db55509","_cell_guid":"7abf7bfe-09e6-4b11-aee1-ee752505c0ab","trusted":true}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import transforms, utils\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom wtfml.utils import EarlyStopping\n\n\n# Load Dataset\nclass Melanoma_Dataset(Dataset):\n    def __init__(self,gt_path, image_path, transform = None, val = False):\n        \n        self.groundtruth = pd.read_csv(gt_path)\n        self.image_path = image_path\n        self.transform = transform\n        \n        self.composed = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        \n    def __len__(self):\n        return len(self.groundtruth)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        img_name = os.path.join(self.image_path,\n                               self.groundtruth.iloc[idx,0])\n        image = io.imread(img_name+\".jpg\")\n        # labels = self.groundtruth.iloc[idx, 1:10]\n        labels = torch.tensor(self.groundtruth.iloc[idx, 1:10].astype(int))\n        # labels = np.array([labels])\n        \n        \n        if self.transform:\n            image = self.transform(image)\n            \n        \n        return image, labels","metadata":{"_uuid":"1031ae80-2c84-4f24-ace0-1e58023140be","_cell_guid":"73a33aab-bdc3-48af-81b0-e01359d3e9fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN","metadata":{"_uuid":"8af91841-5abc-449c-806b-a8cf3a43ad80","_cell_guid":"18796732-e340-4ed8-ab06-0f238eadc862","trusted":true}},{"cell_type":"markdown","source":"### Explanation of the CNN Architecture\n**ImageHeightxImageWidth** (Input to EfficientNet) --> **2560,10,10** (Output of EfficientNet) -(GlobalAvgPooling)-> **2560** (Input for LinearFullyConnected Layer) -> **9** (Output of the Net)","metadata":{"_uuid":"953a0b4f-abd4-44a0-aef6-061eb0dfed99","_cell_guid":"2db1d44e-6267-4aff-8833-546b218cd05b","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom efficientnet_pytorch import EfficientNet\nfrom torch.nn import functional as F\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nimport pretrainedmodels\n\nclass SEResnext50_32x4d(nn.Module):\n    def __init__(self, pretrained= None):\n        super(SEResnext50_32x4d, self).__init__()\n        \n        self.base_model = pretrainedmodels.__dict__[\n            \"se_resnext50_32x4d\"\n        ](pretrained=None)\n        if pretrained is not None:\n            self.base_model.load_state_dict(\n                torch.load(\n                    \"../input/pretrained-model-weights-pytorch/se_resnext50_32x4d-a260b3a4.pth\"\n                )\n            )\n\n        self.dense_output = nn.Linear(2048, 9)\n    \n    def forward(self, image):\n        batch_size, _, _, _ = image.shape\n        \n        x = self.base_model.features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n\n        x = self.dense_output(x)\n\n        return x\n\ndef GlobalAveragePooling(x):\n    return x.mean(axis=-1).mean(axis=-1)\n\nclass MelanomaNet(nn.Module):\n    def __init__(self, pretrained = None):\n        super(MelanomaNet, self).__init__()\n        if pretrained is None:\n            self.efn = EfficientNet.from_pretrained(\"efficientnet-b7\")\n        else:\n            self.efn = EfficientNet.from_name(\"efficientnet-b7\")\n            self.efn.load_state_dict(torch.load(pretrained), strict = False)\n        \n        # Classifier\n        self.avgpool = GlobalAveragePooling\n        self.dense_output = nn.Linear(2560, 9) # length of output vector from EfficientNetb7 is 2560\n        # self.fc2a = nn.Linear(500, 9)\n        \n                \n    def forward(self, x):\n        x = x.view(-1, 3, H , W) # has to be done since EfficientNet is taking such an input\n        x = self.efn.extract_features(x)\n        x = self.avgpool(x)\n        x = self.dense_output(x)\n        return x","metadata":{"_uuid":"afc2646f-670d-462b-b2e4-e7b0e725642e","_cell_guid":"a42cfd98-1a62-41ad-9739-3c7a31d88f1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the training and validation function","metadata":{"_uuid":"0dcaf099-7aea-492b-a52a-bc7ba1a047ff","_cell_guid":"dd81c429-d0c4-4f1f-be8c-b5e5fe79e359","trusted":true}},{"cell_type":"code","source":"def train(model, device, train_loader, optimizer, loss_function, epoch):\n    \n    # Set to training mode\n    model.train()\n    \n    # Loop over all examples\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        # Push to GPU\n        data, target = data.to(device), target.to(device)\n\n        #Reset gradients\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(True):\n\n            #Calculate outputs\n            output = model(data)\n\n\n            # print(\"output:\", output)\n\n\n            # print(\"torch.max(target,1)[1]:\", torch.max(target, 1)[1])\n\n            # Calculate loss (is this correct??)\n            loss = loss_function(output, torch.max(target, 1)[1])\n\n            # Backpropagate loss\n            loss.backward()\n\n            # Apply gradients\n            optimizer.step()\n\n\n        # printout every 50 batches\n        if batch_idx % 50 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n             \n            \n            \ndef val(model, device, test_loader, loss_function, prediction_return=False):\n    # Set to evaluation mode\n    model.eval()\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            curr_loss = loss_function(output,torch.max(target, 1)[1])\n            output = output.cpu().numpy()\n            if i==0:\n                predictions = output\n                \n                targets = target.data.cpu().numpy()\n                \n                loss = np.array([curr_loss.data.cpu().numpy()])\n            else:\n                predictions = np.concatenate((predictions,output))\n                targets = np.concatenate((targets,target.data.cpu().numpy()))\n                loss = np.concatenate((loss,np.array([curr_loss.data.cpu().numpy()])))\n    # One-hot to index:\n    \n    predictions_values = predictions\n    \n    predictions = np.argmax(predictions, 1)\n    \n    targets = np.argmax(targets, 1)\n    \n    # Caluclate metrics\n    accuracy = np.mean(np.equal(predictions,targets))\n    conf_mat = confusion_matrix(targets,predictions)\n    \n    sensitivity = conf_mat.diagonal()/conf_mat.sum(axis=1) # taking the mean calculates the mean sensitivity over ALL classes\n    \n    try:\n        sensitivity_mel = conf_mat[0][0]/conf_mat[0].sum()\n    except:\n        sensitivity_mel = \"No Value\"\n    \n    # Print metrics\n    # print(classification_report(targets, predictions, digits=3))\n    print(\"Test Accuracy:\",accuracy,\"Test Sensitivity (Overall):\",np.mean(sensitivity), \"MEL Sensitivity:\", sensitivity_mel, \"Test loss:\",np.mean(loss))\n    \n    if prediction_return == True:\n        print(\"prediction_return = True\")\n    \n    return predictions, predictions_values, targets\n\n\n\ndef predict(model, device, test_loader, loss_function, prediction_return=False):\n    # Set to evaluation mode\n    model.eval()\n    with torch.no_grad():\n        for i, (data, _) in enumerate(test_loader):\n            data= data.to(device)\n            output = model(data)\n            # curr_loss = loss_function(output,torch.max(target, 1)[1])\n            output = output.cpu().numpy()\n            if i==0:\n                predictions = output\n                \n            else:\n                predictions = np.concatenate((predictions,output))\n    \n    # predictions = np.argmax(predictions, 1)\n    \n    \n    return predictions","metadata":{"_uuid":"6341255c-19d1-4d6f-9e33-2de707c3a40b","_cell_guid":"b64660a1-1d1c-4787-a4c6-bae06dbd4865","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Actual Datasets for the Dataloader","metadata":{"_uuid":"1eade444-f90b-4b4e-9f27-09fde123e491","_cell_guid":"ad5de5cd-e852-4741-998f-5db9bde0579c","trusted":true}},{"cell_type":"markdown","source":"RandomHorizontalFlip(Can be a PIL Image or torch Tensor), RandomVerticalFlip(Can be a PIL Image or torch Tensor), RandomRotation(Can be a PIL Image or torch Tensor),\ntransforms.ColorJitter(Can be a PIL Image or torch Tensor)\n","metadata":{}},{"cell_type":"code","source":"\nmean = [0.485, 0.456, 0.406]\n\nstd = [0.229, 0.224, 0.255]\n\n\n\n# Here we determine what transformation/augmentations we want to have on our image (e.g. flipping, rotating etc.)\ntrain_transform = transforms.Compose([transforms.ToPILImage(),\n                                      transforms.RandomHorizontalFlip(p=0.5),\n                                      transforms.RandomVerticalFlip(p=0.5),\n                                      transforms.RandomApply([transforms.RandomRotation(180), transforms.ColorJitter()], p =0.4),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.255))\n                                     ])\n\n\nval_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.255))])\n\n\n\n\n\n# you have to take the path to \"Test_Data_2019_384\" - I misspelled it\ntrain_dataset = Melanoma_Dataset(gt_path = path_train_gt,\n                                image_path = input_path_training_img,\n                                transform = train_transform)\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size,\n                                           shuffle = True)\n\n\n# you have to take the path to \"Test_Data_2019_384\" - I misspelled it\nval_dataset = Melanoma_Dataset(gt_path = path_val_gt,\n                                image_path = input_path_val_img,\n                                transform = val_transform, val = False)\n\n\n\nval_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size,\n                                           shuffle = False)\n\n\npredict_dataset = Melanoma_Dataset(gt_path = path_predict_gt,\n                                  image_path = input_path_predict_img,\n                                  transform = val_transform, val = False)\n\npredict_loader = torch.utils.data.DataLoader(dataset = predict_dataset, batch_size = batch_size,\n                                           shuffle = False)","metadata":{"_uuid":"8ef200af-ba8f-4fbe-8029-51fdcedfb67e","_cell_guid":"4d32cd5f-8f98-445d-9441-5b8272509bf1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preperation for the actual training","metadata":{"_uuid":"3a9f0a8a-7337-4e3c-beb3-aab24182ad51","_cell_guid":"282d592a-fdc9-41be-b2b8-392daa940f1b","trusted":true}},{"cell_type":"code","source":"import torch.optim as optim\n\n#GPU Info\nprint('Numbers of GPUs: ', torch.cuda.device_count())\nprint('Type of GPUs: ', torch.cuda.get_device_name(device=None))\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Model\n# model = MelanomaNet(pretrained = None).to(device)\n\nmodel = SEResnext50_32x4d()\n\n#model_path = \"../input/model-submission-8/model_8.bin\"\n#model.load_state_dict(torch.load(model_path))\n#model.to(device)\n\n\n\n# Loss\nloss_function = nn.CrossEntropyLoss() #should only be used for 1 class problems??\n# loss_function = nn.NLLLoss()\n\n# Optimizer / we could implement an adaptive learning rate\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\n# Learning Rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, mode=\"max\") #we schedule on the auc of roc curve \n\n# Early Stopping Library\nearly_stopping = EarlyStopping(patience_for_early_stopping, mode=\"max\")\n\n","metadata":{"_uuid":"08931027-9873-4b56-a55a-ee1d7b27a698","_cell_guid":"fa6b118f-9049-4afa-b41b-40b3ce622926","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Where we start training and validate","metadata":{"_uuid":"fcdeecf8-d3a0-4e42-bf25-30d35cc11ec3","_cell_guid":"82d26bd5-be54-4dc4-b1db-6adef2c3be3c","trusted":true}},{"cell_type":"code","source":"n_total_steps = len(train_loader)\n\nprediction_return = True\n\nprint(n_total_steps)\n\nfor epoch in range(num_epochs):\n    train(model, device, train_loader, optimizer, loss_function, epoch)\n        \n    predictions, predictions_values, targets = val(model, device, val_loader, loss_function)\n    \n    ################\n    # Normalize predictions_values for auc\n    # Das hier ausgliedern in neue Funktion!\n    pred_val_del = np.delete(predictions_values, -1, 1) # we have to delete the last entry of all predictions since the index 8 is never appearing in the targets file\n    \n    pred_val_torch = torch.from_numpy(pred_val_del)\n    pred_val_sig = torch.sigmoid(pred_val_torch)\n    row_sums  = torch.sum(pred_val_sig,1)\n    row_sums_full = torch.repeat_interleave(row_sums,8)\n    divider = torch.reshape(row_sums_full, (len(predictions_values),8))\n    y_pred = torch.div( pred_val_sig , divider )\n    y_pred_np = y_pred.numpy()\n\n    ################\n    \n    auc = metrics.roc_auc_score(targets, y_pred_np, multi_class = \"ovr\")\n    \n    print(f\"Epoch = {epoch}, AUC = {auc}\")\n    \n    lr_scheduler.step(auc)\n    \n    early_stopping(auc, model, model_path=f\"model_{epoch}.bin\")\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n    ","metadata":{"_uuid":"482ca47a-a4e9-42b6-bc96-0089c2bf7eae","_cell_guid":"7308e9c0-b9d2-4c30-a598-1ab2e86bbd0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and use a saved model for prediction","metadata":{}},{"cell_type":"code","source":"def predict_SEResnext50():\n    model_path = \"../input/model-submission-8/model_8.bin\"\n    \n    model_pred = SEResnext50_32x4d(pretrained=None)\n    model_pred.load_state_dict(torch.load(model_path))\n    model_pred.to(device)\n    \n    new_predictions = predict(model_pred, device, val_loader, loss_function)\n    # new_predictions = val(model_pred, device, val_loader, loss_function)\n    \n    return new_predictions\n    \n    \ndef predict_EfficientNet():\n    \n    model_path = \"../input/model-7/model_7.bin\"\n    \n    model_pred = MelanomaNet(pretrained = model_path)\n    model_pred.to(device)\n    \n    new_predictions, _, _ = val(model_pred, device, val_loader, loss_function)\n    \n    return new_predictions\n\n\nnew_predictions = predict_SEResnext50()\n# new_predictions = predict_EfficientNet()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(new_predictions))\nprint(len(new_predictions))\nprint(new_predictions)\nprediction_values = new_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the submission csv","metadata":{"_uuid":"53883573-80b8-4625-b5e4-8efe1ff3eed8","_cell_guid":"07f146eb-8c6e-4eb6-8c30-4f7e335625e9","trusted":true}},{"cell_type":"code","source":"# USE new_predictions variable from code block above to create the csv from the predictions made from an uploaded model!!!\nt_p_a = new_predictions\n\noutput_df = pd.read_csv(\"../input/test-gt-empty-sorted/test_gt_empty_sorted.csv\") # VERY IMPORTANT PUT THE GROUND TRUTH CSV FILE PATH WHICH YOU USE FOR THE DATALOADER IN HERE!!!!\nimage_names = output_df.drop(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n\n\npred_df = pd.DataFrame(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n\n\nfor i in range(len(t_p_a)):\n\n    pred_image = np.array([0,0,0,0,0,0,0,0,0])\n    np.put(pred_image, t_p_a[i],1)\n    pred_df = pred_df.append({\"MEL\": pred_image[0], \"NV\": pred_image[1], \"BCC\": pred_image[2],\n                              \"AK\": pred_image[3], \"BKL\": pred_image[4], \"DF\": pred_image[5],\n                              \"VASC\": pred_image[6], \"SCC\": pred_image[7], \"UNK\": pred_image[8]},\n                             ignore_index=True\n                              )    \n\npred_df.insert(loc=0, column=\"image\", value=image_names)\n\nsubmission = pred_df\n\nsubmission.to_csv(\"phase2_submission_group_4.csv\", index=False) # Put the name to the output path as the first parameter\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nimage_names = [\"image\"]\n\ndataframe = pd.DataFrame(columns=['MEL', \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"])\n\ndataframe.insert(loc=0, column=\"image\", value=image_names)\n\ndataframe = dataframe.fillna(0)\n\ndataframe.to_csv(\"single_image_predict.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Print the ROC Curve","metadata":{}},{"cell_type":"markdown","source":"### how they did it in the tutorial (https://scikit-learn.org/0.15/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)","metadata":{}},{"cell_type":"markdown","source":"**Ablauf:**\n\n    1. Lösche aus predictions_values den letzten Eintrag (UNK, 9ter Eintrag, [8]) --> del_pred\n    2. Bringe predictions_values auf die Form (0.6,0.04,0.02,0.18,0.01,0.3,0.01,0.011) alle values zusammen müssen 1 ergeben!\n        a. Das ist unser y_score\n    3. Nutze die gt_val data und bringe sie auf die Form: np.array (1,0,0,0,0,0,0,0)\n        b. Das ist unser y_test\n        \n    4. n_classes = 8 (UNK wird rausgekickt)\n    \n    5. Code wie ein Block weiter oben bzw. wie auf der sklearn Seite ausführen","metadata":{}},{"cell_type":"markdown","source":"## argmax to one hot encoded","metadata":{}},{"cell_type":"code","source":"import scipy\npred = np.delete(prediction_values, -1, 1) \nprint(pred[0])\n\npred_prob = scipy.special.softmax(pred, axis = 1)\nprint(pred_prob[0])\npred_prob.sum(axis= 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_pred = np.delete(predictions_values, -1, 1)  \ndel_pred_max = np.argmax(del_pred, axis = 1)\n\n\ndel_pred_one_hot_enc = np.zeros((del_pred_max.size, del_pred_max.max()+1))\ndel_pred_one_hot_enc[np.arange(del_pred_max.size),del_pred_max] = 1\n\ny_score = del_pred_one_hot_enc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## gt_val data from csv to numpy one hot encoded","metadata":{}},{"cell_type":"code","source":"gt_df = pd.read_csv(\"../input/melanoma-detection-data/groundtruth_val.csv\")\n\ndel gt_df[\"image\"]\n\ngt_np = gt_df.to_numpy()\n\ny_test = gt_np\n\ny_test[:, 3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\n\nclasses = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\"]\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\n\ngt_df = pd.read_csv(\"../input/melanoma-detection-data/groundtruth_val.csv\")\ndel gt_df[\"image\"]\ngt_np = gt_df.to_numpy()\ny_test = gt_np\ny_test[:, 3]\n\ny_score = pred\n\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes)):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfor i in range(len(classes)):\n    plt.plot(fpr[i], tpr[i], label=\"ROC curve of class \" + classes[i] + \"(area = {1:0.2f})\"\n                                   ''.format(i, roc_auc[i]))\n    \nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}